{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a03fd935-0110-4075-b179-b93d01033155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Laad de omgevingsvariabelen uit het .env-bestand\n",
    "load_dotenv()\n",
    "\n",
    "# Haal het basispad op uit de omgevingsvariabele\n",
    "# De .get() methode voorkomt een fout als de variabele niet is ingesteld\n",
    "base_data_dir = os.environ.get('ECOMMERCE_RAW_DATA_DIR', 'default_path_if_not_set/data/ecommerce')\n",
    "\n",
    "# Combineer met de rest van het pad\n",
    "RAW_DATA_PATH = Path(base_data_dir) / 'events.csv'\n",
    "SEQUENCES_PICKLE = 'df_sequences.pkl'\n",
    "PRODUCTS_PICKLE = 'product_df_encoded.pkl'\n",
    "\n",
    "EVENT_COLS = ['event_time', 'user_id', 'user_session', 'product_id', 'event_type']\n",
    "PRODUCT_COLS = ['product_id', 'category_code', 'brand', 'price']\n",
    "SESSION_COLS = ['event_time', 'user_id', 'user_session', 'product_id']\n",
    "CATEGORY_ENCODE_COLS = [\"category_level_1\", \"category_level_2\", \"category_level_3\", \"category_level_4\"]\n",
    "CATEGORICAL_FEATURES = ['brand', 'category_level_1', 'category_level_2', 'category_level_3', 'category_level_4']\n",
    "\n",
    "SOS_ID = 0\n",
    "EOS_ID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5fefda-e105-4beb-9261-617a17c225cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(session_df):\n",
    "    SOS_ID = 0\n",
    "    EOS_ID = 1\n",
    "    \n",
    "    all_sequences = []\n",
    "    # Groepeer per sessie en creëer de sequenties\n",
    "    for user_session, group in session_df.groupby('user_session'):\n",
    "        # Haal de product IDs van de sessie\n",
    "        session_product_ids = group['product_id'].tolist()\n",
    "        \n",
    "        # Voeg SOS en EOS tokens toe\n",
    "        augmented_ids = [SOS_ID] + session_product_ids + [EOS_ID]\n",
    "        \n",
    "        # Genereer de 3-staps sequenties met een sliding window\n",
    "        # Alleen als de sessie lang genoeg is (minstens 1 product)\n",
    "        if len(augmented_ids) >= 3:\n",
    "            for i in range(len(augmented_ids) - 2):\n",
    "                sequence = augmented_ids[i : i + 3]\n",
    "                # Sla de input (eerste 2) en target (laatste) op\n",
    "                all_sequences.append({\n",
    "                    'input_1': sequence[0],\n",
    "                    'input_2': sequence[1],\n",
    "                    'target': sequence[2]\n",
    "                })\n",
    "    return pd.DataFrame(all_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8060fae-0a30-4ec3-989d-91b9ba585150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laden van dataset uit pickle bestanden...\n",
      "Dataset succesvol geladen.\n"
     ]
    }
   ],
   "source": [
    "# --- Helper functies\n",
    "def _load_raw_data(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Laadt de ruwe dataset en filtert op 'view' events.\"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    return df.loc[df.event_type == 'view', :].copy()\n",
    "\n",
    "def _process_session_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Verwerkt sessiegerelateerde kolommen en creëert sequenties.\"\"\"\n",
    "    session_df = df[SESSION_COLS].copy()\n",
    "    session_df['event_time'] = pd.to_datetime(session_df['event_time'])\n",
    "    session_df['user_id'] = session_df['user_id'].astype('category')\n",
    "    session_df['user_session'] = session_df['user_session'].astype('category') # Dubbele regel, kan één keer\n",
    "    \n",
    "    session_df.dropna(inplace=True)\n",
    "    session_df[\"event_week\"] = session_df.event_time.dt.strftime(\"%Y%U\")\n",
    "    session_df = session_df.sort_values(['event_week', 'user_session', 'event_time']).reset_index(drop=True)\n",
    "    \n",
    "    # create_sequences moet hier beschikbaar zijn of geïmporteerd worden.\n",
    "    # Voorbeeld placeholder:\n",
    "    # df_sequences = create_sequences(session_df) \n",
    "    # Aannemende dat 'create_sequences' elders is gedefinieerd en een DataFrame retourneert.\n",
    "    # Hier retourneren we de verwerkte session_df als placeholder voor df_sequences\n",
    "    return session_df \n",
    "\n",
    "def _process_product_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Verwerkt productgerelateerde kolommen en voert one-hot encoding uit.\"\"\"\n",
    "    product_df = df[PRODUCT_COLS].copy().drop_duplicates().set_index('product_id')\n",
    "\n",
    "    product_df['brand'] = product_df['brand'].fillna('UNKNOWN').astype('category')\n",
    "    \n",
    "    # Handling category_code\n",
    "    product_df['category_code'] = product_df['category_code'].astype('string').fillna('UNKNOWN')\n",
    "    \n",
    "    # Log transform en standaardiseer prijs\n",
    "    product_df['price_log'] = np.log1p(product_df['price'])\n",
    "    product_df['price_log_std'] = zscore(product_df['price_log'])\n",
    "    product_df.drop(columns=['price', 'price_log'], inplace=True)\n",
    "    \n",
    "    # Splits en encodeer categorieën\n",
    "    max_levels = product_df['category_code'].apply(lambda x: len(x.split('.'))).max()\n",
    "    column_names = [f'category_level_{i+1}' for i in range(max_levels)]\n",
    "    split_categories = product_df['category_code'].str.split('.', expand=True)\n",
    "    split_categories.columns = column_names[:len(split_categories.columns)] # Zorg voor correcte toewijzing\n",
    "\n",
    "    product_df = pd.concat([product_df, split_categories], axis=1)\n",
    "    \n",
    "    if 'category_code' in product_df.columns:\n",
    "        product_df.drop(columns=['category_code'], inplace=True)\n",
    "        \n",
    "    for col in CATEGORY_ENCODE_COLS:\n",
    "        if col in product_df.columns: # Controleer of de kolom bestaat\n",
    "            product_df[col] = product_df[col].fillna('UNKNOWN').astype('category')\n",
    "        else:\n",
    "            # Voeg lege kolom toe als deze ontbreekt om fouten te voorkomen bij get_dummies\n",
    "            product_df[col] = 'UNKNOWN'\n",
    "            product_df[col] = product_df[col].astype('category')\n",
    "\n",
    "\n",
    "    product_df_encoded = pd.get_dummies(\n",
    "        product_df, \n",
    "        columns=[col for col in CATEGORICAL_FEATURES if col in product_df.columns], \n",
    "        prefix=[col for col in CATEGORICAL_FEATURES if col in product_df.columns], \n",
    "        sparse=True\n",
    "    )\n",
    "    return product_df_encoded\n",
    "\n",
    "# --- Hoofdfunctie\n",
    "def load_dataset(path: Path = RAW_DATA_PATH, refresh: bool = False) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Laadt of genereert de sequenties en productinformatie.\n",
    "\n",
    "    Args:\n",
    "        path (Path): Pad naar de ruwe data CSV.\n",
    "        refresh (bool): Indien True, verwerk de ruwe data opnieuw; anders, laad van pickle bestanden.\n",
    "\n",
    "    Returns:\n",
    "        tuple[pd.DataFrame, pd.DataFrame]: Een tuple met (df_sequences, product_df_encoded).\n",
    "    \"\"\"\n",
    "    if refresh:\n",
    "        print(\"Vernieuwen van dataset...\")\n",
    "        df_raw = _load_raw_data(path)\n",
    "        \n",
    "        df_sequences = _process_session_data(df_raw.copy()) # Gebruik .copy() om SettingWithCopyWarning te voorkomen\n",
    "        product_df_encoded = _process_product_data(df_raw.copy()) # Gebruik .copy()\n",
    "        \n",
    "        # Sla de resultaten op\n",
    "        df_sequences.to_pickle(SEQUENCES_PICKLE)\n",
    "        product_df_encoded.to_pickle(PRODUCTS_PICKLE)\n",
    "        print(\"Dataset vernieuwd en opgeslagen.\")\n",
    "    else:\n",
    "        print(\"Laden van dataset uit pickle bestanden...\")\n",
    "        try:\n",
    "            df_sequences = pd.read_pickle(SEQUENCES_PICKLE)\n",
    "            product_df_encoded = pd.read_pickle(PRODUCTS_PICKLE)\n",
    "            print(\"Dataset succesvol geladen.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Pickle bestanden niet gevonden. Vernieuw de dataset door refresh=True te zetten.\")\n",
    "            # Optioneel: roep de functie recursief aan met refresh=True\n",
    "            df_sequences, product_df_encoded = load_dataset(path=path, refresh=True)\n",
    "    \n",
    "    return df_sequences, product_df_encoded\n",
    "\n",
    "df_sequences, product_df_encoded = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3888c8c-6c23-4116-9868-fa3a1a7f6edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af1e253-40ce-4936-9bbf-6b2540243e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_commerce(df, validation_size=0.15, test_size=0.1):\n",
    "    num_rows = df.shape[0]\n",
    "    test_index = int((1 - test_size) * num_rows)\n",
    "    validation_index = int((1 - test_size - validation_size) * num_rows)\n",
    "\n",
    "    train_set = df.loc[:validation_index, ]\n",
    "    validation_set = df.loc[validation_index:test_index,]\n",
    "    test_set = df.loc[test_index:]\n",
    "    return (train_set, validation_set, test_set)\n",
    "\n",
    "train_set, validation_set, test_set = train_test_split_commerce(df_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76177275-ef2d-40f4-8020-0b4e49a1a3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start voorbereiding graafcomponenten...\n",
      "Mapping gecreëerd: 53452 producten + 2 speciale tokens = 53454 nodes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jjzd8\\AppData\\Local\\Temp\\ipykernel_13936\\658850132.py:60: FutureWarning: Allowing arbitrary scalar fill_value in SparseDtype is deprecated. In a future version, the fill_value must be a valid value for the SparseDtype.subtype.\n",
      "  x_features[2:] = product_df_encoded.loc[real_product_ids_in_order].to_numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature matrix X shape: (53454, 1139)\n",
      "✅ Graafcomponenten succesvol gecreëerd.\n",
      "Totaal nodes in graaf: 53454\n"
     ]
    }
   ],
   "source": [
    "def create_id_to_index_mapping(\n",
    "    product_df_encoded: pd.DataFrame, \n",
    "    sos_id: int, \n",
    "    eos_id: int\n",
    ") -> Tuple[Dict[int, int], int]:\n",
    "    \"\"\"\n",
    "    Creëert een mapping van product-ID's en speciale tokens naar matrix-indices.\n",
    "\n",
    "    Args:\n",
    "        product_df_encoded (pd.DataFrame): DataFrame met productkenmerken, met product_id als index.\n",
    "        sos_id (int): Het ID voor het Start-of-Sequence token.\n",
    "        eos_id (int): Het ID voor het End-of-Sequence token.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, int], int]: Een tuple met de mapping (id_to_index) en het totale aantal nodes (n_nodes).\n",
    "    \"\"\"\n",
    "    unique_product_ids = product_df_encoded.index.tolist()\n",
    "    n_nodes = len(unique_product_ids) + 2  # +2 voor SOS en EOS tokens\n",
    "\n",
    "    id_to_index = {\n",
    "        sos_id: 0,\n",
    "        eos_id: 1\n",
    "    }\n",
    "    # Map alle echte product-ID's naar indices startend bij 2\n",
    "    for i, pid in enumerate(unique_product_ids):\n",
    "        id_to_index[pid] = i + 2\n",
    "    \n",
    "    print(f\"Mapping gecreëerd: {len(unique_product_ids)} producten + 2 speciale tokens = {n_nodes} nodes.\")\n",
    "    return id_to_index, n_nodes\n",
    "\n",
    "def create_node_feature_matrix(\n",
    "    product_df_encoded: pd.DataFrame, \n",
    "    id_to_index: Dict[int, int], \n",
    "    n_nodes: int\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Creëert de node feature matrix (X) voor alle nodes.\n",
    "\n",
    "    Args:\n",
    "        product_df_encoded (pd.DataFrame): DataFrame met productkenmerken, met product_id als index.\n",
    "        id_to_index (Dict[int, int]): Mapping van product-ID's naar matrix-indices.\n",
    "        n_nodes (int): Totaal aantal nodes in de graaf.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: De node feature matrix.\n",
    "    \"\"\"\n",
    "    # Creëer een nulmatrix voor alle nodes\n",
    "    # De eerste twee rijen (indices 0 en 1) voor speciale tokens blijven nullen\n",
    "    x_features = np.zeros((n_nodes, product_df_encoded.shape[1]))\n",
    "\n",
    "    # Plaats de echte productkenmerken in de matrix startend vanaf index 2\n",
    "    # Zorg ervoor dat de volgorde van product_df_encoded overeenkomt met de mapping\n",
    "    # Dit wordt gedaan door te reindexeren op de unieke product-ID's die in de mapping zitten\n",
    "    # (exclusief de speciale tokens)\n",
    "    real_product_ids_in_order = [pid for pid, idx in sorted(id_to_index.items(), key=lambda item: item[1]) if idx >= 2]\n",
    "    \n",
    "    if len(real_product_ids_in_order) != product_df_encoded.shape[0]:\n",
    "        print(\"Waarschuwing: Aantal echte product-ID's in mapping komt niet overeen met product_df_encoded rijen.\")\n",
    "\n",
    "    x_features[2:] = product_df_encoded.loc[real_product_ids_in_order].to_numpy()\n",
    "    \n",
    "    print(f\"Node feature matrix X shape: {x_features.shape}\")\n",
    "    return x_features\n",
    "\n",
    "def create_adjacency_matrix(\n",
    "    df_sequences: pd.DataFrame,\n",
    "    id_to_index: Dict[int, int],\n",
    "    n_nodes: int\n",
    ") -> coo_matrix:\n",
    "    \"\"\"\n",
    "    Creëert de gewogen adjacentie matrix (A) op basis van sessie-sequenties.\n",
    "\n",
    "    De functie is geoptimaliseerd door pandas operaties te combineren en side-effects\n",
    "    zoals 'print' te vervangen door logging.\n",
    "\n",
    "    Args:\n",
    "        df_sequences (pd.DataFrame): DataFrame met kolommen 'input_1' en 'input_2'.\n",
    "        id_to_index (Dict[int, int]): Mapping van product-ID's naar matrix-indices.\n",
    "        n_nodes (int): Totaal aantal nodes in de graaf.\n",
    "\n",
    "    Returns:\n",
    "        coo_matrix: De gewogen adjacentie matrix in COO-formaat.\n",
    "    \"\"\"\n",
    "    # 1. Map, filter en cast de indices in één chained operatie.\n",
    "    #    Dit is efficiënter en beter leesbaar dan losse stappen.\n",
    "    edge_counts = (\n",
    "        df_sequences[['input_1', 'input_2']]\n",
    "        .assign(\n",
    "            from_idx=lambda df: df['input_1'].map(id_to_index),\n",
    "            to_idx=lambda df: df['input_2'].map(id_to_index)\n",
    "        )\n",
    "        .dropna()\n",
    "        .astype({'from_idx': 'int32', 'to_idx': 'int32'})\n",
    "        .groupby(['from_idx', 'to_idx'])\n",
    "        .size()\n",
    "        .reset_index(name='weight')\n",
    "    )\n",
    "\n",
    "    # 2. Transformeer de gewichten met log1p voor numerieke stabiliteit.\n",
    "    log_weights = np.log1p(edge_counts['weight'].to_numpy())\n",
    "\n",
    "    # 3. Creëer de sparse matrix direct vanuit de DataFrame kolommen.\n",
    "    a_weighted = coo_matrix(\n",
    "        (log_weights, (edge_counts['from_idx'].to_numpy(), edge_counts['to_idx'].to_numpy())),\n",
    "        shape=(n_nodes, n_nodes)\n",
    "    )\n",
    "    \n",
    "    return a_weighted\n",
    "\n",
    "def prepare_graph_components(\n",
    "    df_sequences: pd.DataFrame, \n",
    "    product_df_encoded: pd.DataFrame, \n",
    "    sos_id: int, \n",
    "    eos_id: int\n",
    ") -> Tuple[np.ndarray, coo_matrix, Dict[int, int], int]:\n",
    "    \"\"\"\n",
    "    Bereidt alle benodigde graafcomponenten voor: node features (X) en adjacentie matrix (A).\n",
    "\n",
    "    Args:\n",
    "        df_sequences (pd.DataFrame): DataFrame met sessie-sequenties.\n",
    "        product_df_encoded (pd.DataFrame): DataFrame met gecodeerde productkenmerken.\n",
    "        sos_id (int): Het ID voor het Start-of-Sequence token.\n",
    "        eos_id (int): Het ID voor het End-of-Sequence token.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, coo_matrix, Dict[int, int], int]: Een tuple met\n",
    "        (x_features, a_weighted, id_to_index, n_nodes).\n",
    "    \"\"\"\n",
    "    print(\"Start voorbereiding graafcomponenten...\")\n",
    "\n",
    "    id_to_index, n_nodes = create_id_to_index_mapping(product_df_encoded, sos_id, eos_id)\n",
    "    x_features = create_node_feature_matrix(product_df_encoded, id_to_index, n_nodes)\n",
    "    a_weighted = create_adjacency_matrix(df_sequences, id_to_index, n_nodes)\n",
    "\n",
    "    y_labels = df_sequences['target'].map(id_to_index).to_numpy().astype(int)\n",
    "\n",
    "    print(\"✅ Graafcomponenten succesvol gecreëerd.\")\n",
    "    print(f\"Totaal nodes in graaf: {n_nodes}\")\n",
    "    return x_features, a_weighted, y_labels,id_to_index, n_nodes\n",
    "\n",
    "\n",
    "x_features, a_weighted, y_labels, id_to_index, n_nodes = prepare_graph_components(\n",
    "    train_set, \n",
    "    product_df_encoded, \n",
    "    SOS_ID, \n",
    "    EOS_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c770c1e8-f869-4b0a-b936-51a473cc54b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(\"saved_models\", exist_ok=True)\n",
    "os.makedirs(\"saved_plots\", exist_ok=True)\n",
    "os.makedirs(\"logs/fit\", exist_ok=True)\n",
    "\n",
    "def plot_training_curve(history_dict: dict, plot_save_path: str, run_name: str) -> None:\n",
    "    \"\"\"\n",
    "    Visualizes and saves the training and validation curves for loss and accuracy.\n",
    "\n",
    "    Args:\n",
    "        history_dict (dict): Dictionary containing training history (e.g., 'loss', 'val_loss', 'accuracy', 'val_accuracy').\n",
    "        plot_save_path (str): Full path to save the plot image (e.g., 'path/to/plot.png').\n",
    "        run_name (str): Name of the training run for plot titles.\n",
    "    \"\"\"\n",
    "    print(\"Visualizing training curve...\")\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history_dict.get('loss', []), label='Training Loss')\n",
    "    if 'val_loss' in history_dict:\n",
    "        plt.plot(history_dict['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{run_name} - Loss over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history_dict.get('accuracy', []), label='Training Accuracy')\n",
    "    if 'val_accuracy' in history_dict:\n",
    "        plt.plot(history_dict['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title(f'{run_name} - Accuracy over Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout() # Adjust layout to prevent overlapping\n",
    "    plt.savefig(plot_save_path)\n",
    "    plt.close() # Close the plot to free up memory\n",
    "    print(f\"Training curve saved to: {plot_save_path}\")\n",
    "\n",
    "def save_training_results_to_csv(history_dict: dict, run_name: str, run_timestamp: str,\n",
    "                                 batch_size: int, epochs: int, learning_rate: float,\n",
    "                                 results_csv_path: str = 'training_results.csv') -> None:\n",
    "    \"\"\"\n",
    "    Logs training results to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        history_dict (dict): Dictionary containing training history.\n",
    "        run_name (str): Unique name for the training run.\n",
    "        run_timestamp (str): Timestamp of the training run.\n",
    "        batch_size (int): Batch size used for training.\n",
    "        epochs (int): Number of epochs trained.\n",
    "        learning_rate (float): Learning rate used for the optimizer.\n",
    "        results_csv_path (str): Path to the CSV file where results will be stored.\n",
    "    \"\"\"\n",
    "    print(f\"Logging results to {results_csv_path}...\")\n",
    "    results = {\n",
    "        'run_name': run_name,\n",
    "        'timestamp': run_timestamp,\n",
    "        'batch_size': batch_size,\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'final_train_loss': history_dict['loss'][-1],\n",
    "        'final_train_accuracy': history_dict['accuracy'][-1],\n",
    "        'final_val_loss': history_dict['val_loss'][-1] if 'val_loss' in history_dict else None,\n",
    "        'final_val_accuracy': history_dict['val_accuracy'][-1] if 'val_accuracy' in history_dict else None,\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame([results])\n",
    "    if not os.path.exists(results_csv_path):\n",
    "        results_df.to_csv(results_csv_path, index=False)\n",
    "    else:\n",
    "        results_df.to_csv(results_csv_path, mode='a', header=False, index=False)\n",
    "    print(f\"Results added to: {results_csv_path}\")\n",
    "\n",
    "    print(\"\\nAll training results:\")\n",
    "    all_results_df = pd.read_csv(results_csv_path)\n",
    "    print(all_results_df.to_string())\n",
    "\n",
    "def create_run_name(model_name: str, arch_id: str, remark: str,\n",
    "                    batch_size: int, epochs: int, learning_rate: float) -> str:\n",
    "    \"\"\"\n",
    "    Generates a standardized run name for logging and saving.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model (e.g., 'GCN', 'GAT').\n",
    "        arch_id (str): Identifier for the specific architecture.\n",
    "        remark (str): Any additional remark for the run.\n",
    "        batch_size (int): Batch size used.\n",
    "        epochs (int): Number of epochs.\n",
    "        learning_rate (float): Learning rate.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted run name.\n",
    "    \"\"\"\n",
    "    return f\"{model_name}_{arch_id}_{remark}_b{batch_size}_e{epochs}_lr{learning_rate:.6f}\" # Format LR for consistency\n",
    "\n",
    "def train_and_log_spektral_model(\n",
    "    model: tf.keras.Model,\n",
    "    model_name: str,\n",
    "    arch_id: str,\n",
    "    remark: str,\n",
    "    train_X,\n",
    "    train_y,\n",
    "    val_X,\n",
    "    val_y,\n",
    "    batch_size: int = 32,\n",
    "    epochs: int = 50,\n",
    "    learning_rate: float = 0.001,\n",
    "    base_log_dir: str = \"logs/fit\",\n",
    "    base_model_save_dir: str = \"saved_models\",\n",
    "    base_plot_save_dir: str = \"saved_plots\",\n",
    "    results_csv_path: str = \"training_results.csv\"\n",
    ") -> dict or None:\n",
    "    \"\"\"\n",
    "    Trains a Spektral model, logs its progress, and saves results.\n",
    "\n",
    "    Args:\n",
    "        model (tf.keras.Model): The Spektral (Keras) model to train.\n",
    "        model_name (str): Name of the model.\n",
    "        arch_id (str): Architecture identifier.\n",
    "        remark (str): Additional remark for the run.\n",
    "        train_X: Training input data.\n",
    "        train_y: Training target data.\n",
    "        val_X: Validation input data.\n",
    "        val_y: Validation target data.\n",
    "        batch_size (int): Batch size for training.\n",
    "        epochs (int): Number of training epochs.\n",
    "        learning_rate (float): Learning rate for the optimizer.\n",
    "        base_log_dir (str): Base directory for TensorBoard logs.\n",
    "        base_model_save_dir (str): Base directory for saving trained models.\n",
    "        base_plot_save_dir (str): Base directory for saving training plots.\n",
    "        results_csv_path (str): Path to the CSV file for logging training metadata.\n",
    "\n",
    "    Returns:\n",
    "        dict or None: The history dictionary if training is successful, otherwise None.\n",
    "    \"\"\"\n",
    "    run_name = create_run_name(model_name, arch_id, remark, batch_size, epochs, learning_rate)\n",
    "    run_timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "    # Define callbacks dynamically for each run\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.01, # Adjusted factor for more aggressive reduction, original was 0.33, which is quite small\n",
    "            patience=5,\n",
    "            min_lr=1e-7, # Adjusted min_lr\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=10,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        tf.keras.callbacks.TensorBoard(\n",
    "            log_dir=os.path.join(base_log_dir, f\"{run_name}_{run_timestamp}\"),\n",
    "            histogram_freq=1\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    print(\"Compiling model...\")\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy', # Assuming classification; adjust if regression\n",
    "        metrics=['accuracy'] # Adjust metrics based on your problem\n",
    "    )\n",
    "\n",
    "    print(f\"[{run_timestamp}] Starting training for run: {run_name}...\")\n",
    "    try:\n",
    "        history = model.fit(\n",
    "            x=train_X,\n",
    "            y=train_y,\n",
    "            validation_data=(val_X, val_y),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=1,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        print(\"Training completed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during training for run '{run_name}': {e}\")\n",
    "        # Consider re-raising the exception or logging it more formally\n",
    "        return None\n",
    "\n",
    "    history_dict = history.history\n",
    "\n",
    "    # Save model\n",
    "    final_model_save_path = os.path.join(base_model_save_dir, f\"{run_name}_best.keras\")\n",
    "    model.save(final_model_save_path)\n",
    "    print(f\"Model saved to: {final_model_save_path}\")\n",
    "\n",
    "    # Plot training curves\n",
    "    plot_save_path = os.path.join(base_plot_save_dir, f\"{run_name}_training_curve.png\")\n",
    "    plot_training_curve(history_dict, plot_save_path, run_name)\n",
    "\n",
    "    # Save results to CSV\n",
    "    save_training_results_to_csv(history_dict, run_name, run_timestamp,\n",
    "                                 batch_size, epochs, learning_rate, results_csv_path)\n",
    "\n",
    "    print(f\"[{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}] Finished training: {run_name}. \"\n",
    "          f\"Model saved in {final_model_save_path}, graph saved in {plot_save_path}\")\n",
    "\n",
    "    return history_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bbb3e7-0eae-4e77-9147-9bbeedf281fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = None # Aantal knooppunten (kan variabel zijn in een batch)\n",
    "F = node_feature_dim # Dimensie van knooppuntkenmerken\n",
    "\n",
    "X_in = Input(shape=(F,), name='X_in')\n",
    "# Voor Spektral heb je ook een adjacency matrix input nodig\n",
    "# A_in = Input(shape=(N,), sparse=True, name='A_in')\n",
    "\n",
    "# x = GCNConv(32, activation='relu')([X_in, A_in])\n",
    "# x = GlobalAvgPool()(x) # Als je graaf-level output wilt\n",
    "output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Voor een echt Spektral model: model = Model(inputs=[X_in, A_in], outputs=output)\n",
    "model = Model(inputs=X_in, outputs=output) # Voor dummy data\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
